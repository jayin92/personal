<!doctype html><html lang><head><meta name=generator content="Hugo 0.134.0"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>Jie-Ying Lee 李杰穎</title>
<meta name=description content="Jie-Ying Lee's personal website"><meta name=author content='Jie-Ying Lee 李杰穎'><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,100;0,300;0,400;0,700;0,900;1,100;1,300;1,400;1,700;1,900&display=swap" rel=stylesheet><link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/bootstrap/4.6.2/css/bootstrap.min.css integrity="sha512-rt/SrQ4UNIaGfDyEXZtNcyWvQeOq0QLygHluFQcSjaGB04IxWhal71tKuzP6K8eYXYB6vJV4pHkXcmFGGQ1/0w==" crossorigin=anonymous referrerpolicy=no-referrer><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css integrity="sha512-iecdLmaskl7CVkqkXNQ/ZH/XLlvWZOJyj7Yy7tcenmpD1ypASozpmT/E0iPtmFIB46ZmdtAc9eNBvH0H/ZpiBw==" crossorigin=anonymous referrerpolicy=no-referrer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css><link rel=stylesheet href=/css/custom.css><script src=/js/teaser-hover.js></script><link rel=stylesheet href=/sass/researcher.min.css><link rel=icon type=image/ico href=https://jayinnn.dev/favicon.ico><link rel=alternate type=application/rss+xml href=https://jayinnn.dev/index.xml title="Jie-Ying Lee 李杰穎"><script async src="https://www.googletagmanager.com/gtag/js?id=G-XFCYQ29SLK"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-XFCYQ29SLK")}</script></head><body><div class="container mt-5"><nav class="navbar navbar-expand-sm flex-column flex-sm-row text-nowrap p-0"><a class="navbar-brand mx-0 mr-sm-auto" href=https://jayinnn.dev/ title="Jie-Ying Lee 李杰穎">Jie-Ying Lee 李杰穎</a><div class="navbar-nav flex-row flex-wrap justify-content-center"><a class="nav-item nav-link" href=/ title=About>About
</a><span class="nav-item navbar-text mx-1">/</span>
<a class="nav-item nav-link" href=https://raw.githubusercontent.com/jayin92/CV/main/cv.pdf title=Resume><svg width="1em" height="1em" viewBox="0 0 16 16" class="bi bi-file-earmark-person-fill" fill="currentcolor"><path fill-rule="evenodd" d="M2 2a2 2 0 012-2h5.293A1 1 0 0110 .293L13.707 4a1 1 0 01.293.707V14a2 2 0 01-2 2H4a2 2 0 01-2-2V2zm7.5 1.5v-2l3 3h-2a1 1 0 01-1-1zM11 8A3 3 0 115 8a3 3 0 016 0zm2 5.755S12 12 8 12s-5 1.755-5 1.755V14a1 1 0 001 1h8a1 1 0 001-1v-.245z"/></svg> Resume</a></div></nav></div><hr><div id=content><div class=container><figure class=avatar><img src=/avatar.png alt=avatar></figure><p>Ph.D. Student<br>Department of Computer Science<br>National Yang Ming Chiao Tung University, Taiwan</p><p>Software Engineer, Pixel Camera Team, Google</p><p>Email: <a href=mailto:jayinnn.cs14@nycu.edu.tw>jayinnn.cs14@nycu.edu.tw</a></p><p><a href=https://raw.githubusercontent.com/jayin92/CV/main/cv.pdf>CV</a> / <a href="https://scholar.google.com/citations?view_op=list_works&amp;hl=zh-TW&amp;user=mKB6voEAAAAJ">Google Scholar</a> / <a href=https://www.linkedin.com/in/jayinnn/>LinkedIn</a> / <a href=http://github.com/jayin92>GitHub</a> / <a href=https://blog.jayinnn.dev/>Personal Blog</a></p><h2 id=about-me>About Me</h2><p>Hi! I&rsquo;m Jie-Ying (Jay) Lee. I am a Ph.D. student in Computer Science at National Yang Ming Chiao Tung University, advised by <a href=https://yulunalexliu.github.io/>Prof. Yu-Lun Liu</a>. I am also a Software Engineer on Google&rsquo;s Pixel Camera Team, where I develop on-device algorithms for camera.</p><p>I received my B.S. in Computer Science from National Yang Ming Chiao Tung University. During my undergraduate studies, I was an exchange student at ETH Zurich.</p><p>In Summer 2024, I interned with Google&rsquo;s Pixel Camera Team, where I integrated the Segment Anything Model (SAM) for mobile devices, hosted by <a href="https://scholar.google.com/citations?user=0O9rukQAAAAJ&amp;hl=en">Yu-Lin Chang</a> and <a href=https://www.linkedin.com/in/chungkaihsieh>Chung-Kai Hsieh</a>. My industry experience also includes positions as an R&amp;D Intern at Microsoft and a Backend Engineer Intern at Appier.</p><p>I am actively seeking research collaborations.</p><p>Outside of work and research, I enjoy badminton, dance, and <a href=https://www.instagram.com/photograbear_/>photography</a>.</p><h2 id=research-interest>Research Interest</h2><ul><li><strong>3D Reconstruction</strong><ul><li>Neural Radiance Fields (NeRFs)</li><li>3D Gaussian Splatting</li><li>Large-scale Scene Reconstruction</li><li>Satellite Imagery</li></ul></li><li><strong>3D Generation</strong><ul><li>Urban Scene Generation</li><li>Object Generation and Manipulation</li></ul></li><li><strong>Image Segmentation</strong><ul><li>Segment Anything Model</li></ul></li></ul><h2 id=news>News</h2><ul><li>Sep. 2025: Joined Google as a Software Engineer on the Pixel Camera Team!</li><li>Sep. 2025: Started my Ph.D. journey at NYCU with <a href=https://yulunalexliu.github.io/>Prof. Yu-Lun Liu</a>!</li><li>Jul. 2025: One paper accepted to CoRL 2025: &ldquo;See, Point, Fly&rdquo;!</li><li>Jun. 2025: One paper accepted to ICCV 2025: &ldquo;LightsOut&rdquo;!</li><li>Mar. 2025: Two papers accepted to CVPR 2025: &ldquo;AuraFusion360&rdquo; and &ldquo;SpectroMotion&rdquo;!</li><li>Sep. 2024: Began exchange studies in the Department of Computer Science (D-INFK) at ETH Zurich</li><li>Aug. 2024: Co-authored &ldquo;BoostMVSNeRFs&rdquo; paper accepted to the ECCV 2024 Wild3D Workshop as a poster presentation</li><li>Jul. 2024: My work, BEVGaussian, won the <a href=https://www.cs.nycu.edu.tw/storage/materials/xeXTWKdsG4IkteKZGx3lxO6WdeZv4Qi0mgaomFJr.pdf>陽明交大資工系專題競賽佳作 (3rd place of the NYCU CS Undergraduate Research Competition)</a>!</li><li>Jun. 2024: My work, Unbounded Scene Generation, are awarded the <a href="https://www.nstc.gov.tw/folksonomy/list/2af9ad9a-1f47-450d-b5a1-2cb43de8290c?l=ch">國科會大專學生研究計畫 (NSTC Research Grant for University Students)</a>!</li><li>May 2024: Co-authored &ldquo;BoostMVSNeRFs&rdquo; paper accepted to SIGGRAPH 2024!</li></ul><h2 id=publications>Publications</h2><div class=publication-with-teaser><div class=publication-teaser><div class=teaser-container><img src=/images/publications/skyfallgs-teaser.png alt="Skyfall-GS: Synthesizing Immersive 3D Urban Scenes from Satellite Imagery teaser image" class=teaser-image>
<video class=teaser-video muted loop playsinline>
<source src=/videos/publications/skyfallgs-teaser.mp4 type=video/mp4>Your browser does not support the video tag.</video></div></div><div class=publication-content><h3><a href=https://skyfall-gs.jayinnn.dev/ target=_blank rel="noopener noreferrer">Skyfall-GS: Synthesizing Immersive 3D Urban Scenes from Satellite Imagery</a></h3><p class=authors><strong>Jie-Ying Lee</strong>,&nbsp;<a href=https://www.linkedin.com/in/yi-ruei-liu target=_blank rel="noopener noreferrer">Yi-Ruei Liu</a>,&nbsp;<a href=https://orcid.org/0009-0002-2707-0095 target=_blank rel="noopener noreferrer">Shr-Ruei Tsai</a>,&nbsp;<a href="https://openreview.net/profile?id=~Wei-Cheng_Chang3" target=_blank rel="noopener noreferrer">Wei-Cheng Chang</a>,&nbsp;<a href=https://kkennethwu.github.io/ target=_blank rel="noopener noreferrer">Chung-Ho Wu</a>,&nbsp;<a href=https://jiewenchan.github.io/ target=_blank rel="noopener noreferrer">Jiewen Chan</a>,&nbsp;<a href=https://ericzzj1989.github.io/ target=_blank rel="noopener noreferrer">Zhenjun Zhao</a>,&nbsp;<a href=https://hubert0527.github.io/ target=_blank rel="noopener noreferrer">Chieh Hubert Lin</a>,&nbsp;<a href=https://yulunalexliu.github.io/ target=_blank rel="noopener noreferrer">Yu-Lun Liu</a> (*Equal Contribution)</p><p class=venue><em>arXiv, 2025</em></p><div class=publication-abstract>Creating large-scale, photorealistic 3D urban scenes traditionally requires expensive 3D scanning and manual annotation. We present Skyfall-GS, a novel framework that synthesizes city-block scale environments by combining satellite imagery with diffusion models, enabling real-time exploration without costly 3D annotations.</div></div></div><div class=publication-with-teaser><div class=publication-teaser><div class=teaser-container><img src=/images/publications/spf-teaser.jpg alt="See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation teaser image" class=teaser-image>
<video class=teaser-video muted loop playsinline>
<source src=/videos/publications/spf-teaser.mp4 type=video/mp4>Your browser does not support the video tag.</video></div></div><div class=publication-content><h3><a href=https://spf-web.pages.dev/ target=_blank rel="noopener noreferrer">See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation</a></h3><p class=authors><a href=https://orcid.org/0009-0008-5968-6322 target=_blank rel="noopener noreferrer">Chih-Yao Hu</a>*,&nbsp;<a href=https://www.linkedin.com/in/yang-sen-lin/ target=_blank rel="noopener noreferrer">Yang-Sen Lin</a>*,&nbsp;<a href=https://yuna0x0.com/ target=_blank rel="noopener noreferrer">Yuna Lee</a>,&nbsp;<a href=https://su-terry.github.io/ target=_blank rel="noopener noreferrer">Chih-Hai Su</a>,&nbsp;<strong>Jie-Ying Lee</strong>,&nbsp;<a href=https://orcid.org/0009-0002-2707-0095 target=_blank rel="noopener noreferrer">Shr-Ruei Tsai</a>,&nbsp;<a href=https://linjohnss.github.io/ target=_blank rel="noopener noreferrer">Chin-Yang Lin</a>,&nbsp;<a href=https://www.cs.nycu.edu.tw/members/detail/kuanwen target=_blank rel="noopener noreferrer">Kuan-Wen Chen</a>,&nbsp;<a href=https://twke18.github.io/ target=_blank rel="noopener noreferrer">Tsung-Wei Ke</a>,&nbsp;<a href=https://yulunalexliu.github.io/ target=_blank rel="noopener noreferrer">Yu-Lun Liu</a> (*Equal Contribution)</p><p class=venue><em>The Conference on Robot Learning <strong>(CoRL)</strong>, 2025</em></p><div class=publication-abstract>This work presents See, Point, Fly (SPF), a training-free aerial vision-and-language navigation (AVLN) framework built atop vision-language models (VLMs), to consider action prediction for AVLN as a 2D spatial grounding task.</div></div></div><div class=publication-with-teaser><div class=publication-teaser><div class=teaser-container><img src=/images/publications/aurafusion360-teaser.jpg alt="AuraFusion360: Augmented Unseen Region Alignment for Reference-based 360° Unbounded Scene Inpainting teaser image" class=teaser-image>
<video class=teaser-video muted loop playsinline>
<source src=/videos/publications/aurafusion360-teaser.mp4 type=video/mp4>Your browser does not support the video tag.</video></div></div><div class=publication-content><h3><a href=https://kkennethwu.github.io/aurafusion360/ target=_blank rel="noopener noreferrer">AuraFusion360: Augmented Unseen Region Alignment for Reference-based 360° Unbounded Scene Inpainting</a></h3><p class=authors><a href=https://kkennethwu.github.io/ target=_blank rel="noopener noreferrer">Chung-Ho Wu</a>*,&nbsp;Yang-Jung Chen*,&nbsp;Ying-Huan Chen,&nbsp;<strong>Jie-Ying Lee</strong>,&nbsp;<a href=https://hentci.github.io/ target=_blank rel="noopener noreferrer">Bo-Hsu Ke</a>,&nbsp;<a href=https://www.linkedin.com/in/ray-tuan-mu-a46257246/ target=_blank rel="noopener noreferrer">Chun-Wei Tuan Mu</a>,&nbsp;Yi-Chuan Huang,&nbsp;<a href=https://linjohnss.github.io/ target=_blank rel="noopener noreferrer">Chin-Yang Lin</a>,&nbsp;<a href=https://minhungchen.netlify.app/ target=_blank rel="noopener noreferrer">Min-Hung Chen</a>,&nbsp;<a href=https://sites.google.com/site/yylinweb/ target=_blank rel="noopener noreferrer">Yen-Yu Lin</a>,&nbsp;<a href=https://yulunalexliu.github.io/ target=_blank rel="noopener noreferrer">Yu-Lun Liu</a> (*Equal Contribution)</p><p class=venue><em>IEEE/CVF Conference on Computer Vision and Pattern Recognition <strong>(CVPR)</strong>, 2025</em></p><div class=publication-abstract>The approach introduces (1) depth-aware unseen mask generation for accurate occlusion identification, (2) Adaptive Guided Depth Diffusion, a zero-shot method for accurate initial point placement without requiring additional training, and (3) SDEdit-based detail enhancement for multi-view coherence.</div></div></div><div class=publication-with-teaser><div class=publication-teaser><div class=teaser-container><img src=/images/publications/spectromotion-teaser.jpg alt="SpectroMotion: Dynamic 3D Reconstruction of Specular Scenes teaser image" class=teaser-image>
<video class=teaser-video muted loop playsinline>
<source src=/videos/publications/spectromotion-teaser.mp4 type=video/mp4>Your browser does not support the video tag.</video></div></div><div class=publication-content><h3><a href=https://cdfan0627.github.io/spectromotion/ target=_blank rel="noopener noreferrer">SpectroMotion: Dynamic 3D Reconstruction of Specular Scenes</a></h3><p class=authors><a href=https://cdfan0627.github.io/ target=_blank rel="noopener noreferrer">Cheng-De Fan</a>,&nbsp;Chen-Wei Chang,&nbsp;<a href=https://www.linkedin.com/in/yi-ruei-liu target=_blank rel="noopener noreferrer">Yi-Ruei Liu</a>,&nbsp;<strong>Jie-Ying Lee</strong>,&nbsp;<a href=https://people.cs.nycu.edu.tw/~jlhuang/ target=_blank rel="noopener noreferrer">Jiun-Long Huang</a>,&nbsp;<a href=https://sites.google.com/view/yctseng target=_blank rel="noopener noreferrer">Yu-Chee Tseng</a>,&nbsp;<a href=https://yulunalexliu.github.io/ target=_blank rel="noopener noreferrer">Yu-Lun Liu</a></p><p class=venue><em>IEEE/CVF Conference on Computer Vision and Pattern Recognition <strong>(CVPR)</strong>, 2025</em></p><div class=publication-abstract>SpectroMotion is presented, a novel approach that combines 3D Gaussian Splatting with physically-based rendering (PBR) and deformation fields to reconstruct dynamic specular scenes and is the only existing 3DGS method capable of synthesizing photorealistic real-world dynamic specular scenes.</div></div></div><div class=publication-with-teaser><div class=publication-teaser><div class=teaser-container><img src=/images/publications/boostmvsnerfs-teaser.jpg alt="BoostMVSNeRFs: Boosting MVS-based NeRFs to Generalizable View Synthesis in Large-scale Scenes teaser image" class=teaser-image>
<video class=teaser-video muted loop playsinline>
<source src=/videos/publications/boostmvsnerfs-teaser.mp4 type=video/mp4>Your browser does not support the video tag.</video></div></div><div class=publication-content><h3><a href=https://su-terry.github.io/BoostMVSNeRFs/ target=_blank rel="noopener noreferrer">BoostMVSNeRFs: Boosting MVS-based NeRFs to Generalizable View Synthesis in Large-scale Scenes</a></h3><p class=authors><a href=https://su-terry.github.io/ target=_blank rel="noopener noreferrer">Chih-Hai Su</a>*,&nbsp;<a href=https://orcid.org/0009-0008-5968-6322 target=_blank rel="noopener noreferrer">Chih-Yao Hu</a>*,&nbsp;<a href=https://orcid.org/0009-0002-2707-0095 target=_blank rel="noopener noreferrer">Shr-Ruei Tsai</a>*,&nbsp;<strong>Jie-Ying Lee</strong>*,&nbsp;<a href=https://linjohnss.github.io/ target=_blank rel="noopener noreferrer">Chin-Yang Lin</a>,&nbsp;<a href=https://yulunalexliu.github.io/ target=_blank rel="noopener noreferrer">Yu-Lun Liu</a> (*Equal Contribution)</p><p class=venue><em>ACM Special Interest Group on Computer Graphics and Interactive Techniques <strong>(SIGGRAPH)</strong>, 2024</em></p><div class=publication-abstract>This paper presents a novel approach called BoostMVSNeRFs to enhance the rendering quality of MVS-based NeRFs in large-scale scenes, and identifies limitations in MVS-based NeRF methods, such as restricted viewport coverage and artifacts due to limited input views.</div></div></div></div></div><div id=footer class=mb-5><hr><div class="container text-center"><a href=https://fb.me/jayinnn/ class="fab fa-facebook fa-1x" title=Facebook></a><a href=https://github.com/jayin92/ class="fab fa-github fa-1x" title=GitHub></a><a href=https://www.linkedin.com/in/jayinnn/ class="fab fa-linkedin fa-1x" title=LinkedIn></a><a href=mailto:jayinnn.cs10@nycu.edu.tw class="fas fa-envelope fa-1x" title=E-mail></a><a href="https://scholar.google.com/citations?user=mKB6voEAAAAJ" class="ai ai-google-scholar fa-1x" title=Scholar></a></div><div class="container text-center"><a href=https://jayinnn.dev/ title="By Jie-Ying Lee 李杰穎"><small>By Jie-Ying Lee 李杰穎</small></a></div></div></body></html>