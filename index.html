<!doctype html><html lang><head><meta name=generator content="Hugo 0.134.0"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>Jie-Ying Lee 李杰穎</title>
<meta name=description content="Jie-Ying Lee's personal website"><meta name=author content='Jie-Ying Lee 李杰穎'><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,100;0,300;0,400;0,700;0,900;1,100;1,300;1,400;1,700;1,900&display=swap" rel=stylesheet><link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/bootstrap/4.6.2/css/bootstrap.min.css integrity="sha512-rt/SrQ4UNIaGfDyEXZtNcyWvQeOq0QLygHluFQcSjaGB04IxWhal71tKuzP6K8eYXYB6vJV4pHkXcmFGGQ1/0w==" crossorigin=anonymous referrerpolicy=no-referrer><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css integrity="sha512-iecdLmaskl7CVkqkXNQ/ZH/XLlvWZOJyj7Yy7tcenmpD1ypASozpmT/E0iPtmFIB46ZmdtAc9eNBvH0H/ZpiBw==" crossorigin=anonymous referrerpolicy=no-referrer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css><link rel=stylesheet href=/css/custom.css><script src=/js/teaser-hover.js></script><link rel=stylesheet href=/sass/researcher.min.css><link rel=icon type=image/ico href=https://jayinnn.dev/favicon.ico><link rel=alternate type=application/rss+xml href=https://jayinnn.dev/index.xml title="Jie-Ying Lee 李杰穎"><script async src="https://www.googletagmanager.com/gtag/js?id=G-XFCYQ29SLK"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-XFCYQ29SLK")}</script></head><body><div class="container mt-5" style=display:none><nav class="navbar navbar-expand-sm flex-column flex-sm-row text-nowrap p-0"><a class="navbar-brand mx-0 mr-sm-auto" href=https://jayinnn.dev/ title="Jie-Ying Lee 李杰穎">Jie-Ying Lee 李杰穎</a><div class="navbar-nav flex-row flex-wrap justify-content-center"></div></nav></div><div id=content><div class=container><div class=header-section><div class=header-text><p><span class=large-name>Jie-Ying Lee 李杰穎</span></p><p>Ph.D. Student @ NYCU CS<br>Software Engineer @ Google Pixel Camera</p><p>Email: <a href=mailto:jayinnn.cs14@nycu.edu.tw>jayinnn.cs14@nycu.edu.tw</a></p><p><a href="https://mozilla.github.io/pdf.js/web/viewer.html?file=https://raw.githubusercontent.com/jayin92/CV/main/cv.pdf">CV</a> / <a href="https://scholar.google.com/citations?view_op=list_works&amp;hl=zh-TW&amp;user=mKB6voEAAAAJ">Scholar</a> / <a href=https://www.linkedin.com/in/jayinnn/>LinkedIn</a> / <a href=http://github.com/jayin92>GitHub</a> / <a href=https://x.com/jayinnn>X</a> / <a href=https://threads.com/@jayinnn>Threads</a> / <a href=https://blog.jayinnn.dev/>Blog</a></p></div><figure class=avatar><img src=/avatar.png alt=avatar></figure></div><h2 id=about-me>About Me</h2><div class=intro-box><p>I&rsquo;m a Ph.D. student in <a href="https://www.cs.nycu.edu.tw/?locale=en">Computer Science</a> at National Yang Ming Chiao Tung University, advised by <a href=https://yulunalexliu.github.io/>Prof. Yu-Lun Liu</a>, and a Software Engineer on Google&rsquo;s Pixel Camera Team. I work on <strong>3D scene synthesis</strong>, <strong>generative models for vision</strong>, and <strong>embodied AI</strong>, particularly focusing on <strong>Neural Radiance Fields</strong>, <strong>3D Gaussian Splatting</strong>, <strong>vision-language navigation</strong>, and <strong>on-device perception</strong>.</p><p>I received my B.S. in Computer Science from National Yang Ming Chiao Tung University, with an exchange semester at ETH Zurich. My industry experience includes internships at Google (Pixel Camera Team), Microsoft, and Appier.</p><p><strong>I am actively seeking research collaborations.</strong> If you are interested in working with me, don&rsquo;t hesitate to reach out.</p><div class=affiliation-logos><div class=affiliation-item><img src=/google.png alt=Google>
<span class=affiliation-name>Google</span>
<span class=affiliation-title>SWE (2025 - Present)</span></div><div class=affiliation-item><img src=/nycu.png alt=NYCU>
<span class=affiliation-name>NYCU</span>
<span class=affiliation-title>Ph.D. Student (2025 - Present)</span>
<span class=affiliation-title>B.S in Computer Science (2021 - 2025)</span></div><div class=affiliation-item><img src=/eth.png alt="ETH Zurich">
<span class=affiliation-name>ETH Zurich</span>
<span class=affiliation-title>Exchange Student (2024 - 2025)</span></div></div></div><h2 id=news>News</h2><ul><li><strong>Sep. 2025:</strong> Joined <a href=https://about.google/>Google</a> as a Software Engineer on the Pixel Camera Team and started my Ph.D. at NYCU with <a href=https://yulunalexliu.github.io/>Prof. Yu-Lun Liu</a></li><li><strong>Jul. 2025:</strong> <a href=https://spf-web.pages.dev/>See, Point, Fly</a> accepted to <strong>CoRL 2025</strong></li><li><strong>Jun. 2025:</strong> <a href=https://ray-1026.github.io/lightsout/>LightsOut</a> accepted to <strong>ICCV 2025</strong></li><li><strong>Mar. 2025:</strong> Two papers accepted to <strong>CVPR 2025</strong>: <a href=https://kkennethwu.github.io/aurafusion360/>AuraFusion360</a> and <a href=https://cdfan0627.github.io/spectromotion/>SpectroMotion</a></li><li><strong>Sep. 2024:</strong> Exchange semester at ETH Zurich, D-INFK (<a href=https://blog.jayinnn.dev/posts/eth/>blog post</a>)</li><li><strong>Jul. 2024:</strong> BEVGaussian won <a href=https://www.cs.nycu.edu.tw/storage/materials/xeXTWKdsG4IkteKZGx3lxO6WdeZv4Qi0mgaomFJr.pdf>NYCU CS Undergraduate Research Competition (3rd place)</a></li><li><strong>Jun. 2024:</strong> Awarded <a href="https://www.nstc.gov.tw/folksonomy/list/2af9ad9a-1f47-450d-b5a1-2cb43de8290c?l=ch">NSTC Research Grant for University Students</a></li><li><strong>May 2024:</strong> <a href=https://su-terry.github.io/BoostMVSNeRFs/>BoostMVSNeRFs</a> accepted to <strong>SIGGRAPH 2024</strong></li></ul><h2 id=publications>Publications</h2><div class=publication-with-teaser><div class=publication-teaser><div class=teaser-container><img src=/images/publications/skyfallgs-teaser.png alt="Skyfall-GS: Synthesizing Immersive 3D Urban Scenes from Satellite Imagery teaser image" class=teaser-image>
<span class=teaser-hint>▶ Hover / Tap</span>
<video class=teaser-video muted loop playsinline>
<source src=/videos/publications/skyfallgs-teaser.mp4 type=video/mp4>Your browser does not support the video tag.
</video>
<script>(function(){const e=document.currentScript.parentElement,t=e.querySelector("video.teaser-video");if(t){let o,i=!1;const a="ontouchstart"in window||navigator.maxTouchPoints>0;function n(){i=!0,e.classList.add("active"),o=t.play()}function s(){i=!1,e.classList.remove("active"),o!==void 0&&o.then(function(){t.pause(),t.currentTime=0}).catch(function(){})}a||(e.addEventListener("mouseenter",n),e.addEventListener("mouseleave",s)),e.addEventListener("touchend",function(e){e.preventDefault(),i?s():n()})}})()</script></div></div><div class=publication-content><div class=publication-meta><a href=https://skyfall-gs.jayinnn.dev/ target=_blank rel="noopener noreferrer" class=publication-title>Skyfall-GS: Synthesizing Immersive 3D Urban Scenes from Satellite Imagery</a><br><span class=authors><strong>Jie-Ying Lee</strong>,&nbsp;<a href=https://www.linkedin.com/in/yi-ruei-liu target=_blank rel="noopener noreferrer">Yi-Ruei Liu</a>,&nbsp;<a href=https://orcid.org/0009-0002-2707-0095 target=_blank rel="noopener noreferrer">Shr-Ruei Tsai</a>,&nbsp;<a href="https://openreview.net/profile?id=~Wei-Cheng_Chang3" target=_blank rel="noopener noreferrer">Wei-Cheng Chang</a>,&nbsp;<a href=https://kkennethwu.github.io/ target=_blank rel="noopener noreferrer">Chung-Ho Wu</a>,&nbsp;<a href=https://jiewenchan.github.io/ target=_blank rel="noopener noreferrer">Jiewen Chan</a>,&nbsp;<a href=https://ericzzj1989.github.io/ target=_blank rel="noopener noreferrer">Zhenjun Zhao</a>,&nbsp;<a href=https://hubert0527.github.io/ target=_blank rel="noopener noreferrer">Chieh Hubert Lin</a>,&nbsp;<a href=https://yulunalexliu.github.io/ target=_blank rel="noopener noreferrer">Yu-Lun Liu</a></span><br><em class=venue>arXiv, 2025</em><br><span class=publication-links><a href=https://skyfall-gs.jayinnn.dev/ target=_blank rel="noopener noreferrer">project page</a> / <a href=https://arxiv.org/abs/2510.15869 target=_blank rel="noopener noreferrer">arXiv</a> / <a href=https://arxiv.org/pdf/2510.15869 target=_blank rel="noopener noreferrer">pdf</a> / <a href=https://github.com/jayin92/skyfall-gs target=_blank rel="noopener noreferrer">code</a> / <a href=https://youtu.be/zj2-aGSe6ao target=_blank rel="noopener noreferrer">video</a></span></div><div class=publication-abstract>We present Skyfall-GS, a framework that synthesizes photorealistic, city-block scale 3D urban scenes from satellite imagery using diffusion models, eliminating the need for expensive 3D scanning and manual annotation while enabling real-time exploration.</div></div></div><div class=publication-with-teaser><div class=publication-teaser><div class=teaser-container><img src=/images/publications/lightsout-teaser-input.jpg alt="LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal teaser image" class=teaser-image>
<span class=teaser-hint>▶ Hover / Tap</span>
<img src=/images/publications/lightsout-teaser-output.jpg alt="LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal hover image" class="teaser-hover-image teaser-video">
<script>(function(){const e=document.currentScript.parentElement;e.addEventListener("click",function(){e.classList.toggle("active")})})()</script></div></div><div class=publication-content><div class=publication-meta><a href=https://ray-1026.github.io/lightsout/ target=_blank rel="noopener noreferrer" class=publication-title>LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal</a><br><span class=authors><a href=https://orcid.org/0009-0002-2707-0095 target=_blank rel="noopener noreferrer">Shr-Ruei Tsai</a>,&nbsp;<a href="https://openreview.net/profile?id=~Wei-Cheng_Chang3" target=_blank rel="noopener noreferrer">Wei-Cheng Chang</a>,&nbsp;<strong>Jie-Ying Lee</strong>,&nbsp;<a href=https://su-terry.github.io/ target=_blank rel="noopener noreferrer">Chih-Hai Su</a>,&nbsp;<a href=https://yulunalexliu.github.io/ target=_blank rel="noopener noreferrer">Yu-Lun Liu</a></span><br><em class=venue>ICCV, 2025</em><br><span class=publication-links><a href=https://ray-1026.github.io/lightsout/ target=_blank rel="noopener noreferrer">project page</a> / <a href=https://arxiv.org/abs/2510.15868 target=_blank rel="noopener noreferrer">arXiv</a> / <a href=https://github.com/Ray-1026/LightsOut-official target=_blank rel="noopener noreferrer">code</a> / <a href=https://youtu.be/VNzJ9Z_dJNI target=_blank rel="noopener noreferrer">video</a> / <a href=https://huggingface.co/spaces/RayTsai-030/LightsOut-demo target=_blank rel="noopener noreferrer">demo</a></span></div><div class=publication-abstract>We present LightsOut, a diffusion-based outpainting framework that enhances lens flare removal by reconstructing off-frame light sources. Our approach combines a multitask regression module with LoRA fine-tuned diffusion models to produce realistic and physically consistent results.</div></div></div><div class=publication-with-teaser><div class=publication-teaser><div class=teaser-container><img src=/images/publications/spf-teaser.jpg alt="See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation teaser image" class=teaser-image>
<span class=teaser-hint>▶ Hover / Tap</span>
<video class=teaser-video muted loop playsinline>
<source src=/videos/publications/spf-teaser.mp4 type=video/mp4>Your browser does not support the video tag.
</video>
<script>(function(){const e=document.currentScript.parentElement,t=e.querySelector("video.teaser-video");if(t){let o,i=!1;const a="ontouchstart"in window||navigator.maxTouchPoints>0;function n(){i=!0,e.classList.add("active"),o=t.play()}function s(){i=!1,e.classList.remove("active"),o!==void 0&&o.then(function(){t.pause(),t.currentTime=0}).catch(function(){})}a||(e.addEventListener("mouseenter",n),e.addEventListener("mouseleave",s)),e.addEventListener("touchend",function(e){e.preventDefault(),i?s():n()})}})()</script></div></div><div class=publication-content><div class=publication-meta><a href=https://spf-web.pages.dev/ target=_blank rel="noopener noreferrer" class=publication-title>See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation</a><br><span class=authors><a href=https://orcid.org/0009-0008-5968-6322 target=_blank rel="noopener noreferrer">Chih-Yao Hu</a>*,&nbsp;<a href=https://www.linkedin.com/in/yang-sen-lin/ target=_blank rel="noopener noreferrer">Yang-Sen Lin</a>*,&nbsp;<a href=https://yuna0x0.com/ target=_blank rel="noopener noreferrer">Yuna Lee</a>,&nbsp;<a href=https://su-terry.github.io/ target=_blank rel="noopener noreferrer">Chih-Hai Su</a>,&nbsp;<strong>Jie-Ying Lee</strong>,&nbsp;<a href=https://orcid.org/0009-0002-2707-0095 target=_blank rel="noopener noreferrer">Shr-Ruei Tsai</a>,&nbsp;<a href=https://linjohnss.github.io/ target=_blank rel="noopener noreferrer">Chin-Yang Lin</a>,&nbsp;<a href=https://www.cs.nycu.edu.tw/members/detail/kuanwen target=_blank rel="noopener noreferrer">Kuan-Wen Chen</a>,&nbsp;<a href=https://twke18.github.io/ target=_blank rel="noopener noreferrer">Tsung-Wei Ke</a>,&nbsp;<a href=https://yulunalexliu.github.io/ target=_blank rel="noopener noreferrer">Yu-Lun Liu</a> (*Equal Contribution)</span><br><em class=venue>CoRL, 2025</em><br><span class=publication-links><a href=https://spf-web.pages.dev/ target=_blank rel="noopener noreferrer">project page</a> / <a href=https://arxiv.org/abs/2509.22653 target=_blank rel="noopener noreferrer">arXiv</a> / <a href=https://github.com/Hu-chih-yao/see-point-fly target=_blank rel="noopener noreferrer">code</a> / <a href=https://youtu.be/EQPBkNhAuyU target=_blank rel="noopener noreferrer">video</a></span></div><div class=publication-abstract>We present See, Point, Fly (SPF), a training-free framework for aerial vision-and-language navigation. By leveraging vision-language models and reformulating navigation as a 2D spatial grounding task, SPF enables universal unmanned aerial navigation without task-specific training.</div></div></div><div class=publication-with-teaser><div class=publication-teaser><div class=teaser-container><img src=/images/publications/aurafusion360-teaser.jpg alt="AuraFusion360: Augmented Unseen Region Alignment for Reference-based 360° Unbounded Scene Inpainting teaser image" class=teaser-image>
<span class=teaser-hint>▶ Hover / Tap</span>
<video class=teaser-video muted loop playsinline>
<source src=/videos/publications/aurafusion360-teaser.mp4 type=video/mp4>Your browser does not support the video tag.
</video>
<script>(function(){const e=document.currentScript.parentElement,t=e.querySelector("video.teaser-video");if(t){let o,i=!1;const a="ontouchstart"in window||navigator.maxTouchPoints>0;function n(){i=!0,e.classList.add("active"),o=t.play()}function s(){i=!1,e.classList.remove("active"),o!==void 0&&o.then(function(){t.pause(),t.currentTime=0}).catch(function(){})}a||(e.addEventListener("mouseenter",n),e.addEventListener("mouseleave",s)),e.addEventListener("touchend",function(e){e.preventDefault(),i?s():n()})}})()</script></div></div><div class=publication-content><div class=publication-meta><a href=https://kkennethwu.github.io/aurafusion360/ target=_blank rel="noopener noreferrer" class=publication-title>AuraFusion360: Augmented Unseen Region Alignment for Reference-based 360° Unbounded Scene Inpainting</a><br><span class=authors><a href=https://kkennethwu.github.io/ target=_blank rel="noopener noreferrer">Chung-Ho Wu</a>*,&nbsp;Yang-Jung Chen*,&nbsp;Ying-Huan Chen,&nbsp;<strong>Jie-Ying Lee</strong>,&nbsp;<a href=https://hentci.github.io/ target=_blank rel="noopener noreferrer">Bo-Hsu Ke</a>,&nbsp;<a href=https://www.linkedin.com/in/ray-tuan-mu-a46257246/ target=_blank rel="noopener noreferrer">Chun-Wei Tuan Mu</a>,&nbsp;Yi-Chuan Huang,&nbsp;<a href=https://linjohnss.github.io/ target=_blank rel="noopener noreferrer">Chin-Yang Lin</a>,&nbsp;<a href=https://minhungchen.netlify.app/ target=_blank rel="noopener noreferrer">Min-Hung Chen</a>,&nbsp;<a href=https://sites.google.com/site/yylinweb/ target=_blank rel="noopener noreferrer">Yen-Yu Lin</a>,&nbsp;<a href=https://yulunalexliu.github.io/ target=_blank rel="noopener noreferrer">Yu-Lun Liu</a> (*Equal Contribution)</span><br><em class=venue>CVPR, 2025</em><br><span class=publication-links><a href=https://kkennethwu.github.io/aurafusion360/ target=_blank rel="noopener noreferrer">project page</a> / <a href=https://arxiv.org/abs/2502.05176 target=_blank rel="noopener noreferrer">arXiv</a> / <a href=https://github.com/kkennethwu/AuraFusion360_official.git target=_blank rel="noopener noreferrer">code</a> / <a href="https://www.youtube.com/watch?v=V1_EMXtYhTE" target=_blank rel="noopener noreferrer">video</a></span></div><div class=publication-abstract>We introduce AuraFusion360, a reference-based 360° scene inpainting method with three key innovations: depth-aware occlusion identification, Adaptive Guided Depth Diffusion for zero-shot point placement, and SDEdit-based enhancement for multi-view coherence.</div></div></div><div class=publication-with-teaser><div class=publication-teaser><div class=teaser-container><img src=/images/publications/spectromotion-teaser.jpg alt="SpectroMotion: Dynamic 3D Reconstruction of Specular Scenes teaser image" class=teaser-image>
<span class=teaser-hint>▶ Hover / Tap</span>
<video class=teaser-video muted loop playsinline>
<source src=/videos/publications/spectromotion-teaser.mp4 type=video/mp4>Your browser does not support the video tag.
</video>
<script>(function(){const e=document.currentScript.parentElement,t=e.querySelector("video.teaser-video");if(t){let o,i=!1;const a="ontouchstart"in window||navigator.maxTouchPoints>0;function n(){i=!0,e.classList.add("active"),o=t.play()}function s(){i=!1,e.classList.remove("active"),o!==void 0&&o.then(function(){t.pause(),t.currentTime=0}).catch(function(){})}a||(e.addEventListener("mouseenter",n),e.addEventListener("mouseleave",s)),e.addEventListener("touchend",function(e){e.preventDefault(),i?s():n()})}})()</script></div></div><div class=publication-content><div class=publication-meta><a href=https://cdfan0627.github.io/spectromotion/ target=_blank rel="noopener noreferrer" class=publication-title>SpectroMotion: Dynamic 3D Reconstruction of Specular Scenes</a><br><span class=authors><a href=https://cdfan0627.github.io/ target=_blank rel="noopener noreferrer">Cheng-De Fan</a>,&nbsp;Chen-Wei Chang,&nbsp;<a href=https://www.linkedin.com/in/yi-ruei-liu target=_blank rel="noopener noreferrer">Yi-Ruei Liu</a>,&nbsp;<strong>Jie-Ying Lee</strong>,&nbsp;<a href=https://people.cs.nycu.edu.tw/~jlhuang/ target=_blank rel="noopener noreferrer">Jiun-Long Huang</a>,&nbsp;<a href=https://sites.google.com/view/yctseng target=_blank rel="noopener noreferrer">Yu-Chee Tseng</a>,&nbsp;<a href=https://yulunalexliu.github.io/ target=_blank rel="noopener noreferrer">Yu-Lun Liu</a></span><br><em class=venue>CVPR, 2025</em><br><span class=publication-links><a href=https://cdfan0627.github.io/spectromotion/ target=_blank rel="noopener noreferrer">project page</a> / <a href=https://arxiv.org/abs/2410.17249 target=_blank rel="noopener noreferrer">arXiv</a> / <a href=https://github.com/cdfan0627/SpectroMotion target=_blank rel="noopener noreferrer">code</a> / <a href="https://www.youtube.com/watch?v=yqKLUDIdN9g" target=_blank rel="noopener noreferrer">video</a></span></div><div class=publication-abstract>We present SpectroMotion, the first 3D Gaussian Splatting method capable of reconstructing photorealistic dynamic specular scenes. By combining 3DGS with physically-based rendering and deformation fields, we achieve high-quality synthesis of challenging real-world dynamic reflective surfaces.</div></div></div><div class=publication-with-teaser><div class=publication-teaser><div class=teaser-container><img src=/images/publications/boostmvsnerfs-teaser.jpg alt="BoostMVSNeRFs: Boosting MVS-based NeRFs to Generalizable View Synthesis in Large-scale Scenes teaser image" class=teaser-image>
<span class=teaser-hint>▶ Hover / Tap</span>
<video class=teaser-video muted loop playsinline>
<source src=/videos/publications/boostmvsnerfs-teaser.mp4 type=video/mp4>Your browser does not support the video tag.
</video>
<script>(function(){const e=document.currentScript.parentElement,t=e.querySelector("video.teaser-video");if(t){let o,i=!1;const a="ontouchstart"in window||navigator.maxTouchPoints>0;function n(){i=!0,e.classList.add("active"),o=t.play()}function s(){i=!1,e.classList.remove("active"),o!==void 0&&o.then(function(){t.pause(),t.currentTime=0}).catch(function(){})}a||(e.addEventListener("mouseenter",n),e.addEventListener("mouseleave",s)),e.addEventListener("touchend",function(e){e.preventDefault(),i?s():n()})}})()</script></div></div><div class=publication-content><div class=publication-meta><a href=https://su-terry.github.io/BoostMVSNeRFs/ target=_blank rel="noopener noreferrer" class=publication-title>BoostMVSNeRFs: Boosting MVS-based NeRFs to Generalizable View Synthesis in Large-scale Scenes</a><br><span class=authors><a href=https://su-terry.github.io/ target=_blank rel="noopener noreferrer">Chih-Hai Su</a>*,&nbsp;<a href=https://orcid.org/0009-0008-5968-6322 target=_blank rel="noopener noreferrer">Chih-Yao Hu</a>*,&nbsp;<a href=https://orcid.org/0009-0002-2707-0095 target=_blank rel="noopener noreferrer">Shr-Ruei Tsai</a>*,&nbsp;<strong>Jie-Ying Lee</strong>*,&nbsp;<a href=https://linjohnss.github.io/ target=_blank rel="noopener noreferrer">Chin-Yang Lin</a>,&nbsp;<a href=https://yulunalexliu.github.io/ target=_blank rel="noopener noreferrer">Yu-Lun Liu</a> (*Equal Contribution)</span><br><em class=venue>SIGGRAPH, 2024</em><br><span class=publication-links><a href=https://su-terry.github.io/BoostMVSNeRFs/ target=_blank rel="noopener noreferrer">project page</a> / <a href=https://arxiv.org/abs/2407.15848 target=_blank rel="noopener noreferrer">arXiv</a> / <a href=https://github.com/Su-Terry/BoostMVSNeRFs target=_blank rel="noopener noreferrer">code</a> / <a href="https://www.youtube.com/watch?v=tX4EkFgm0ng" target=_blank rel="noopener noreferrer">video</a></span></div><div class=publication-abstract>We present BoostMVSNeRFs, a method that enhances rendering quality for MVS-based NeRFs in large-scale scenes. Our approach addresses key limitations including restricted viewport coverage and artifacts from limited input views, enabling generalizable view synthesis in complex environments.</div></div></div><h2 id=service>Service</h2><ul><li><strong>Teaching Assistant:</strong> <a href=https://github.com/jayin92/Lab4-FlowMatching>Image and Video Generation</a>, NYCU (2025 Fall)</li></ul><h2 id=misc>Misc.</h2><p>Beyond research, I&rsquo;m passionate about staying active through badminton and hip-hop dance. I also enjoy capturing moments through <a href=https://www.instagram.com/photograbear_/>photography</a>.</p><p>Music-wise, I&rsquo;m into Taiwanese indie and hip-hop, frequently listening to artists like <a href="https://www.youtube.com/watch?v=QvrIk0Ff_DI">Gummy B</a>, <a href=https://youtu.be/j2311FZWoFQ>草東沒有派對 (No Party For Cao Dong)</a>, and <a href=https://youtu.be/BZJLX-jDa9k>國蛋 GorDoN</a>.</p></div></div><div id=footer class=mb-5><hr><div class="container text-center"><a href=https://fb.me/jayinnn/ class="fab fa-facebook fa-1x" title=Facebook></a><a href=https://github.com/jayin92/ class="fab fa-github fa-1x" title=GitHub></a><a href=https://www.linkedin.com/in/jayinnn/ class="fab fa-linkedin fa-1x" title=LinkedIn></a><a href=mailto:jayinnn.cs14@nycu.edu.tw class="fas fa-envelope fa-1x" title=E-mail></a><a href="https://scholar.google.com/citations?user=mKB6voEAAAAJ" class="ai ai-google-scholar fa-1x" title=Scholar></a><a href=https://x.com/jayinnn class="fab fa-x-twitter fa-1x" title=X></a><a href=https://threads.com/@jayinnn class="fab fa-threads fa-1x" title=Threads></a></div><div class="container text-center"><a href=https://jayinnn.dev/ title="By Jie-Ying Lee 李杰穎"><small>By Jie-Ying Lee 李杰穎</small></a></div></div></body></html>